{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(p1, p2):\n",
    "    d = {1:0, 2:0}\n",
    "    for _ in range(100):\n",
    "        winner = init.main(100, 0, [p1, p2])\n",
    "        d[winner] += 1\n",
    "\n",
    "    win_rate = d[1]/100\n",
    "    return win_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.504\n"
     ]
    }
   ],
   "source": [
    "print(score(1, 'Default'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.623\n"
     ]
    }
   ],
   "source": [
    "print(score(0.8, 'Default'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 1ms/step - loss: 0.6933\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6932\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6932\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6932\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6932\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6932\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 976us/step - loss: 0.6931\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.6931\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.6931\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 915us/step - loss: 0.6931\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 898us/step - loss: 0.6931\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 873us/step - loss: 0.6931\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 885us/step - loss: 0.6931\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 894us/step - loss: 0.6931\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 932us/step - loss: 0.6931\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 930us/step - loss: 0.6931\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 970us/step - loss: 0.6931\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 910us/step - loss: 0.6931\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 923us/step - loss: 0.6931\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 929us/step - loss: 0.6931\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 885us/step - loss: 0.6931\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 821us/step - loss: 0.6931\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 966us/step - loss: 0.6931\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 907us/step - loss: 0.6931\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.6931\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 955us/step - loss: 0.6931\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 964us/step - loss: 0.6931\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 926us/step - loss: 0.6931\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 998us/step - loss: 0.6931\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Define the architecture of the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=1, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Generate some training data\n",
    "X_train = np.random.uniform(0, 1, size=(1000, 1))\n",
    "y_train = np.array([score(x, 'Default') for x in X_train])\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Use the neural network to predict the input that maximizes the output of score()\n",
    "def maximize_score():\n",
    "    x = np.random.uniform(0, 1, size=(1, 1))\n",
    "    for i in range(100):\n",
    "        grad = model.predict(x) * (1 - model.predict(x)) * 2 * (score(x, 'Default') - 0.5)\n",
    "        x += grad\n",
    "        x = np.clip(x, 0, 1)\n",
    "    return x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59\n"
     ]
    }
   ],
   "source": [
    "print(score(.78, 'Default'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "[[0.5025745 ]\n",
      " [0.5025745 ]\n",
      " [0.5025745 ]\n",
      " [0.5025745 ]\n",
      " [0.5025745 ]\n",
      " [0.5025745 ]\n",
      " [0.5025745 ]\n",
      " [0.5025745 ]\n",
      " [0.5025745 ]\n",
      " [0.5025745 ]\n",
      " [0.5025745 ]\n",
      " [0.5025923 ]\n",
      " [0.50262684]\n",
      " [0.50266135]\n",
      " [0.5026958 ]\n",
      " [0.5027303 ]\n",
      " [0.50276476]\n",
      " [0.50279933]\n",
      " [0.5028337 ]\n",
      " [0.50286824]\n",
      " [0.50290275]\n",
      " [0.5029372 ]\n",
      " [0.5029717 ]\n",
      " [0.50300616]\n",
      " [0.5030406 ]\n",
      " [0.5030751 ]\n",
      " [0.50310963]\n",
      " [0.5031441 ]\n",
      " [0.5031786 ]\n",
      " [0.50321305]\n",
      " [0.50324756]\n",
      " [0.503282  ]\n",
      " [0.50331646]\n",
      " [0.503351  ]\n",
      " [0.5033855 ]\n",
      " [0.50342   ]\n",
      " [0.5033441 ]\n",
      " [0.5032573 ]\n",
      " [0.50316644]\n",
      " [0.5030756 ]\n",
      " [0.50298476]\n",
      " [0.5028939 ]\n",
      " [0.5028031 ]\n",
      " [0.5027123 ]\n",
      " [0.5026215 ]\n",
      " [0.50253063]\n",
      " [0.5024398 ]\n",
      " [0.50234896]\n",
      " [0.5022581 ]\n",
      " [0.50216734]\n",
      " [0.5020765 ]\n",
      " [0.50198567]\n",
      " [0.50189483]\n",
      " [0.501804  ]\n",
      " [0.50171316]\n",
      " [0.5016223 ]\n",
      " [0.5015315 ]\n",
      " [0.5014407 ]\n",
      " [0.50134987]\n",
      " [0.5012591 ]\n",
      " [0.5011682 ]\n",
      " [0.50107735]\n",
      " [0.50098646]\n",
      " [0.50089574]\n",
      " [0.50080484]\n",
      " [0.50071406]\n",
      " [0.5006232 ]\n",
      " [0.5005323 ]\n",
      " [0.50044155]\n",
      " [0.5003507 ]\n",
      " [0.50025994]\n",
      " [0.50016904]\n",
      " [0.50007826]\n",
      " [0.4999874 ]\n",
      " [0.49989653]\n",
      " [0.4998057 ]\n",
      " [0.4997149 ]\n",
      " [0.49962407]\n",
      " [0.49953324]\n",
      " [0.4994424 ]\n",
      " [0.49935156]\n",
      " [0.49926072]\n",
      " [0.49916992]\n",
      " [0.49907908]\n",
      " [0.4989882 ]\n",
      " [0.49889743]\n",
      " [0.4988066 ]\n",
      " [0.49871576]\n",
      " [0.49862492]\n",
      " [0.49853408]\n",
      " [0.49844328]\n",
      " [0.49835244]\n",
      " [0.4982616 ]\n",
      " [0.49817076]\n",
      " [0.49807996]\n",
      " [0.49798912]\n",
      " [0.49789828]\n",
      " [0.49780744]\n",
      " [0.49771664]]\n"
     ]
    }
   ],
   "source": [
    "input = [i/100 for i in range(1, 100)]\n",
    "print(model.predict(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "0.43276569528122966\n"
     ]
    }
   ],
   "source": [
    "print(maximize_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "The input that produces the highest output is: 0.3535353535353536\n",
      "The highest output is: 0.50339764\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate a range of inputs\n",
    "X_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "# Predict the output for each input\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "# Find the input that produces the highest output\n",
    "x_best = X_test[np.argmax(y_test)]\n",
    "y_best = np.max(y_test)\n",
    "\n",
    "print(\"The input that produces the highest output is:\", x_best[0])\n",
    "print(\"The highest output is:\", y_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(p2, guess):\n",
    "    def objective(x):\n",
    "            return -score(x, p2)\n",
    "\n",
    "    # Define the initial input, the learning rate, and the number of iterations\n",
    "    x = guess\n",
    "    learning_rate = 0.01\n",
    "    num_iterations = 50\n",
    "\n",
    "    # Define the beta parameters for the Adam optimizer\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "\n",
    "    # Define the initial values for the first and second moments\n",
    "    m = 0\n",
    "    v = 0\n",
    "\n",
    "    iter_vals = []\n",
    "\n",
    "    # Optimize the input using Adam optimization\n",
    "    for i in range(num_iterations):\n",
    "        if len(iter_vals) >= 2 and abs(iter_vals[-1] - iter_vals[-2]) < 0.0005:\n",
    "             break\n",
    "\n",
    "        # Compute the gradient of the objective function using numerical differentiation\n",
    "        gradient = (objective(x + 0.001) - objective(x)) / 0.001\n",
    "        # Change the sign of the gradient to maximize the objective function\n",
    "        #gradient = -gradient\n",
    "        # Update the first and second moments\n",
    "        m = beta1 * m + (1 - beta1) * gradient\n",
    "        v = beta2 * v + (1 - beta2) * gradient**2\n",
    "        # Compute the bias-corrected first and second moments\n",
    "        m_hat = m / (1 - beta1**(i+1))\n",
    "        v_hat = v / (1 - beta2**(i+1))\n",
    "        # Update the input using the Adam optimizer\n",
    "        x = x - learning_rate * m_hat / (v_hat**(1/2) + 1e-8)\n",
    "\n",
    "        iter_vals.append(x)\n",
    "\n",
    "        print(x)\n",
    "\n",
    "    return x, score(x, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7900000000032259\n",
      "0.7803978647490034\n",
      "0.7720336223673754\n",
      "0.7693033808419909\n",
      "0.7661561155997165\n",
      "0.7610619569174661\n",
      "0.7543665013021877\n",
      "0.750890637216982\n",
      "0.7488750373711317\n",
      "0.7478987895810434\n",
      "0.7445145350214086\n",
      "0.7417830564643616\n",
      "0.7387802185298251\n",
      "0.7365347932796241\n",
      "0.7337734647097174\n",
      "0.7315788680059311\n",
      "0.7285448874853775\n",
      "0.7266686619259888\n",
      "0.724814687434162\n",
      "0.7242500717627264\n",
      "0.72180240755199\n",
      "0.718161707984016\n",
      "0.715780048867016\n",
      "0.7133891351066971\n",
      "0.7094800446657824\n",
      "0.7083397147369838\n",
      "0.7071016566104281\n",
      "0.7071459040492122\n",
      "(0.7071459040492122, 0.55)\n"
     ]
    }
   ],
   "source": [
    "print(gradient_descent('Default', 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(p2, guess):\n",
    "    def objective(x):\n",
    "            return score(x, p2)\n",
    "\n",
    "    # Define the initial input, the learning rate, and the number of iterations\n",
    "    x = guess\n",
    "    learning_rate = 0.01\n",
    "    num_iterations = 50\n",
    "\n",
    "\n",
    "    iter_vals = []\n",
    "\n",
    "    # Optimize the input using Adam optimization\n",
    "    for i in range(num_iterations):\n",
    "        if len(iter_vals) >= 2 and abs(iter_vals[-1] - iter_vals[-2]) < 0.0005:\n",
    "             break\n",
    "\n",
    "        # Compute the gradient of the objective function using numerical differentiation\n",
    "        gradient = (objective(x + 0.001) - objective(x)) / 0.001\n",
    "       \n",
    "        x += learning_rate * gradient\n",
    "\n",
    "        iter_vals.append(x)\n",
    "\n",
    "        print(x)\n",
    "\n",
    "    return x, score(x, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2800000000000005\n",
      "1.0600000000000014\n",
      "1.3200000000000005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(gradient_descent(\u001b[39m'\u001b[39;49m\u001b[39mDefault\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m0.8\u001b[39;49m))\n",
      "Cell \u001b[0;32mIn[104], line 19\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(p2, guess)\u001b[0m\n\u001b[1;32m     16\u001b[0m      \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Compute the gradient of the objective function using numerical differentiation\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m gradient \u001b[39m=\u001b[39m (objective(x \u001b[39m+\u001b[39m \u001b[39m0.001\u001b[39m) \u001b[39m-\u001b[39m objective(x)) \u001b[39m/\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[1;32m     21\u001b[0m x \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m gradient\n\u001b[1;32m     23\u001b[0m iter_vals\u001b[39m.\u001b[39mappend(x)\n",
      "Cell \u001b[0;32mIn[104], line 3\u001b[0m, in \u001b[0;36mgradient_descent.<locals>.objective\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective\u001b[39m(x):\n\u001b[0;32m----> 3\u001b[0m         \u001b[39mreturn\u001b[39;00m score(x, p2)\n",
      "Cell \u001b[0;32mIn[95], line 4\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(p1, p2)\u001b[0m\n\u001b[1;32m      2\u001b[0m d \u001b[39m=\u001b[39m {\u001b[39m1\u001b[39m:\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m:\u001b[39m0\u001b[39m}\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     winner \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39;49mmain(\u001b[39m1000\u001b[39;49m, \u001b[39m0\u001b[39;49m, [p1, p2])\n\u001b[1;32m      5\u001b[0m     d[winner] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      7\u001b[0m win_rate \u001b[39m=\u001b[39m d[\u001b[39m1\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m1000\u001b[39m\n",
      "File \u001b[0;32m~/Documents/College/Summer 23/Monopoly/init.py:690\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(limit_turns, game_mode, player_modes)\u001b[0m\n\u001b[1;32m    688\u001b[0m rolls \u001b[39m=\u001b[39m []\n\u001b[1;32m    689\u001b[0m \u001b[39mwhile\u001b[39;00m double \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(rolls) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 690\u001b[0m     roll, double \u001b[39m=\u001b[39m game\u001b[39m.\u001b[39;49mdice_roll(\u001b[39mlen\u001b[39;49m(rolls))\n\u001b[1;32m    691\u001b[0m     rolls\u001b[39m.\u001b[39mappend(roll)\n\u001b[1;32m    693\u001b[0m \u001b[39m#print(rolls)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/College/Summer 23/Monopoly/init.py:214\u001b[0m, in \u001b[0;36mGame.dice_roll\u001b[0;34m(self, reroll_count)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdice_roll\u001b[39m(\u001b[39mself\u001b[39m, reroll_count):\n\u001b[1;32m    213\u001b[0m     a \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m1\u001b[39m, \u001b[39m6\u001b[39m)\n\u001b[0;32m--> 214\u001b[0m     b \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mrandint(\u001b[39m1\u001b[39;49m, \u001b[39m6\u001b[39;49m)\n\u001b[1;32m    215\u001b[0m     \u001b[39msum\u001b[39m \u001b[39m=\u001b[39m a \u001b[39m+\u001b[39m b\n\u001b[1;32m    216\u001b[0m     double \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/random.py:358\u001b[0m, in \u001b[0;36mRandom.randint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mempty range for randrange()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m     \u001b[39mreturn\u001b[39;00m istart \u001b[39m+\u001b[39m istep \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_randbelow(n)\n\u001b[0;32m--> 358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandint\u001b[39m(\u001b[39mself\u001b[39m, a, b):\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return random integer in range [a, b], including both end points.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandrange(a, b\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(gradient_descent('Default', 0.8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
