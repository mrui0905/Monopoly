{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be implementing a mean-squared regression and a neural network to determine the optimal aggresion level against a typical opponent. We assume that there are two players, one of which has an aggression rating of 'Default', or 0.5.\n",
    "\n",
    "Training data has been simualted using create_training.ipynb and is stored in '/data.training_data.csv'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To first run a regression, we use an XGBoost ((Extreme Gradient Boosting)) regression model which minimizes the loss function (mean squared error) when fitting the data. Afterwards we're able to determine the input which returns the highest output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# import training data\n",
    "df = pd.read_csv('data/training_data.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1], df.iloc[:, 2], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost regression model\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Reshape the data to fit the model's requirements\n",
    "X_train = np.array(X_train).reshape(-1, 1)\n",
    "X_test = np.array(X_test).reshape(-1, 1)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Generate a range of inputs\n",
    "inputs = np.linspace(0, 1, num=1000).reshape(-1, 1)\n",
    "\n",
    "# Predict the outputs for the inputs using the trained model\n",
    "outputs = model.predict(inputs)\n",
    "\n",
    "# Find the input with the highest predicted output\n",
    "highest_output_index = np.argmax(outputs)\n",
    "highest_output_input = inputs[highest_output_index][0]\n",
    "\n",
    "print(\"Input with the highest output:\", highest_output_input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result seems to be inconsistent with our prior study. While we've certaintly established that higher aggression ratios (conservative approaches) tend to win at a higher rate, the model indicates that a .98 ratio is optimal, i.e. rarely ever executing any trades and/or buying houses/hotels. It seems that this model is too simple to fully fit and model Monopoly.\n",
    "\n",
    "Thus, we'll then create a neural network that predicts the win rate of a aggression ratio to be used to perform gradient descent to optimize our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "df = pd.read_csv('data/training_data.csv')\n",
    "\n",
    "# Load the input-output pairs from the dataframe\n",
    "inputs = df.iloc[:, 1].values\n",
    "outputs = df.iloc[:, 2].values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_inputs = inputs[:800]\n",
    "train_outputs = outputs[:800]\n",
    "val_inputs = inputs[800:]\n",
    "val_outputs = outputs[800:]\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_inputs, train_outputs, epochs=30, batch_size=200, validation_data=(val_inputs, val_outputs))\n",
    "\n",
    "# Use the model to predict the output for a given input\n",
    "input_to_predict = 0.5\n",
    "predicted_output = model.predict(np.array([input_to_predict]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we use a feedforward neural network with multiple hidden layers. This code defines a feedforward neural network with two hidden layers, each with a ReLU activation function. The output layer has a linear activation function, which is appropriate for predicting continuous values. The model is trained using the mean squared error loss function and the Adam optimizer. After training, the model can be used to predict the output for a given input.\n",
    "\n",
    "Now, we can define a gradient descent function to optimize our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(guess):\n",
    "    def objective(x):\n",
    "        input_to_predict = x\n",
    "        predicted_output = model.predict(np.array([input_to_predict]))\n",
    "        return predicted_output[0][0]\n",
    "\n",
    "    # Define the initial input, the learning rate, and the number of iterations\n",
    "    x = guess\n",
    "    learning_rate = 0.01\n",
    "    num_iterations = 100\n",
    "\n",
    "    # Store prior iterations to terminate gradient descent early\n",
    "    iter_vals = []\n",
    "\n",
    "    beta = 0.99\n",
    "    \n",
    "    # Optimize the input using Adam optimization\n",
    "    for i in range(num_iterations):\n",
    "        if len(iter_vals) >= 2 and abs(iter_vals[-1] - iter_vals[-2]) < 0.0005:\n",
    "             break\n",
    "\n",
    "        # Compute the gradient of the objective function using numerical differentiation\n",
    "        gradient = (objective(x + 0.001) - objective(x)) / 0.001\n",
    "       \n",
    "        x += (learning_rate * gradient) * (beta)**i\n",
    "\n",
    "        iter_vals.append(x)\n",
    "\n",
    "        print(x)\n",
    "\n",
    "    return x, objective(x)\n",
    "\n",
    "print(gradient_descent(0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model seems to better represent our monopoly game, optimizing a player's win rate at roughy 0.80. This falls in line with our prior study on aggression.\n",
    "\n",
    "Of course, 0.80 is only optimal in a two person game where the opposing player plays at exactly a 0.50 aggression ratio. If any of these assumptions are violated, we would need to create new trading data through simulations and adjust our models accordingly. However, this study provides the guidlines for these future inquires and can be easily manipulated to fit various differing parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
